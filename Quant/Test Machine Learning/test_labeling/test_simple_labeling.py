#!/usr/bin/env python3
"""
Test simplificado para labeling
Usando las funciones reales disponibles
"""

import sys
import os
import unittest
import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
import json
import logging
from pathlib import Path

# Agregar paths para importar mÃ³dulos
sys.path.append('/workspaces/Sistema-de-datos/Quant/Machine Learning')
sys.path.append('/workspaces/Sistema-de-datos/Quant/Test Machine Learning')

# Importaciones robustas
try:
    from labeling import (
        get_events, apply_pt_sl_on_t1, get_bins, 
        drop_labels, cusum_filter, get_daily_vol
    )
    LABELING_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Warning: Could not import labeling: {e}")
    LABELING_AVAILABLE = False

try:
    from test_config_manager import ConfigurationManager
    CONFIG_MANAGER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Warning: Could not import ConfigurationManager: {e}")
    CONFIG_MANAGER_AVAILABLE = False

warnings.filterwarnings('ignore')

class TestLabelingSimple(unittest.TestCase):
    """Test simplificado para labeling"""
    
    @classmethod
    def setUpClass(cls):
        """ConfiguraciÃ³n inicial para todos los tests"""
        cls.test_results = {
            'timestamp': datetime.now().isoformat(),
            'total_tests': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'execution_time': 0,
            'test_details': {}
        }
        
        if CONFIG_MANAGER_AVAILABLE:
            cls.config_manager = ConfigurationManager()
            cls.config = cls.config_manager.get_config('labeling')
        else:
            cls.config = {'batch_size': 1000, 'test_size': 100}
        
        # Crear datos de prueba
        cls.sample_data = cls._create_sample_data()
        
        print("ðŸ§ª TestLabelingSimple - ConfiguraciÃ³n inicial completada")
    
    @classmethod
    def _create_sample_data(cls) -> pd.DataFrame:
        """Crear datos de muestra para pruebas"""
        print("ðŸ“Š Creando datos de muestra para labeling...")
        
        np.random.seed(42)
        n_samples = 1000
        
        # Generar serie de precios con random walk
        price_changes = np.random.normal(0, 0.01, n_samples)
        prices = 100 + np.cumsum(price_changes)
        
        # Generar timestamps
        start_time = datetime.now() - timedelta(hours=n_samples)
        timestamps = [start_time + timedelta(minutes=i) for i in range(n_samples)]
        
        data = pd.DataFrame({
            'timestamp': timestamps,
            'price': prices,
            'returns': price_changes,
            'volume': np.random.lognormal(mean=10, sigma=1, size=n_samples).astype(int),
            'high': prices * (1 + np.abs(np.random.normal(0, 0.005, n_samples))),
            'low': prices * (1 - np.abs(np.random.normal(0, 0.005, n_samples))),
            'close': prices
        })
        
        # Asegurar que high >= price >= low
        data['high'] = np.maximum(data['high'], data['price'])
        data['low'] = np.minimum(data['low'], data['price'])
        
        data = data.set_index('timestamp')
        
        print(f"âœ… Datos de muestra creados: {len(data)} filas")
        return data
    
    def test_basic_functionality(self):
        """Test bÃ¡sico de funcionalidad"""
        test_name = "test_basic_functionality"
        start_time = datetime.now()
        
        try:
            # Test bÃ¡sico de datos
            self.assertIsInstance(self.sample_data, pd.DataFrame)
            self.assertGreater(len(self.sample_data), 0)
            self.assertIn('price', self.sample_data.columns)
            self.assertIn('returns', self.sample_data.columns)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': True,
                'execution_time': execution_time,
                'data_samples': len(self.sample_data)
            }
            
            print(f"âœ… {test_name} passed - {len(self.sample_data)} samples")
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': False,
                'execution_time': execution_time,
                'error': str(e)
            }
            print(f"âŒ {test_name} failed: {e}")
            raise
    
    def test_volatility_calculation(self):
        """Test bÃ¡sico de cÃ¡lculo de volatilidad"""
        test_name = "test_volatility_calculation"
        start_time = datetime.now()
        
        try:
            # Calcular volatilidad bÃ¡sica
            returns = self.sample_data['returns'].dropna()
            volatility = returns.rolling(window=20).std()
            
            self.assertIsInstance(volatility, pd.Series)
            self.assertGreater(len(volatility.dropna()), 0)
            
            # Test de volatilidad diaria si estÃ¡ disponible
            if LABELING_AVAILABLE:
                try:
                    daily_vol = get_daily_vol(self.sample_data['close'])
                    self.assertIsInstance(daily_vol, pd.Series)
                    print(f"âœ… Daily volatility calculated: {len(daily_vol)} values")
                except Exception as e:
                    print(f"âš ï¸  Daily vol function error: {e}")
            
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': True,
                'execution_time': execution_time,
                'volatility_samples': len(volatility.dropna())
            }
            
            print(f"âœ… {test_name} passed - {len(volatility.dropna())} volatility values")
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': False,
                'execution_time': execution_time,
                'error': str(e)
            }
            print(f"âŒ {test_name} failed: {e}")
            raise
    
    def test_simple_labeling(self):
        """Test bÃ¡sico de labeling"""
        test_name = "test_simple_labeling"
        start_time = datetime.now()
        
        try:
            # Crear labels simples basados en retornos
            returns = self.sample_data['returns']
            
            # Labels binarios: 1 si retorno positivo, 0 si negativo
            binary_labels = (returns > 0).astype(int)
            
            self.assertIsInstance(binary_labels, pd.Series)
            self.assertGreater(len(binary_labels), 0)
            
            # Verificar que tenemos ambos tipos de labels
            unique_labels = binary_labels.unique()
            self.assertIn(0, unique_labels)
            self.assertIn(1, unique_labels)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': True,
                'execution_time': execution_time,
                'labels_created': len(binary_labels),
                'positive_labels': int(binary_labels.sum()),
                'negative_labels': int(len(binary_labels) - binary_labels.sum())
            }
            
            print(f"âœ… {test_name} passed - {len(binary_labels)} labels created")
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': False,
                'execution_time': execution_time,
                'error': str(e)
            }
            print(f"âŒ {test_name} failed: {e}")
            raise
    
    def test_triple_barrier_concept(self):
        """Test bÃ¡sico del concepto de triple barrier"""
        test_name = "test_triple_barrier_concept"
        start_time = datetime.now()
        
        try:
            # Implementar triple barrier simplificado
            prices = self.sample_data['close']
            
            # Definir barriers
            pt = 0.02  # profit taking
            sl = 0.02  # stop loss
            
            labels = []
            for i in range(len(prices) - 1):
                current_price = prices.iloc[i]
                future_prices = prices.iloc[i+1:i+21]  # 20 perÃ­odos adelante
                
                if len(future_prices) == 0:
                    labels.append(0)
                    continue
                
                # Calcular retornos
                returns = (future_prices / current_price) - 1
                
                # Verificar barriers
                if (returns >= pt).any():
                    labels.append(1)  # profit target hit
                elif (returns <= -sl).any():
                    labels.append(-1)  # stop loss hit
                else:
                    labels.append(0)  # time barrier
            
            labels.append(0)  # Ãºltimo perÃ­odo
            
            self.assertEqual(len(labels), len(prices))
            
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': True,
                'execution_time': execution_time,
                'labels_created': len(labels),
                'profit_labels': labels.count(1),
                'loss_labels': labels.count(-1),
                'neutral_labels': labels.count(0)
            }
            
            print(f"âœ… {test_name} passed - {len(labels)} triple barrier labels")
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': False,
                'execution_time': execution_time,
                'error': str(e)
            }
            print(f"âŒ {test_name} failed: {e}")
            raise
    
    def test_statistical_analysis(self):
        """Test bÃ¡sico de anÃ¡lisis estadÃ­stico"""
        test_name = "test_statistical_analysis"
        start_time = datetime.now()
        
        try:
            # AnÃ¡lisis estadÃ­stico bÃ¡sico
            returns = self.sample_data['returns']
            
            # EstadÃ­sticas bÃ¡sicas
            mean_return = returns.mean()
            std_return = returns.std()
            skewness = returns.skew()
            kurtosis = returns.kurtosis()
            
            # Verificaciones bÃ¡sicas
            self.assertIsInstance(mean_return, float)
            self.assertIsInstance(std_return, float)
            self.assertGreater(std_return, 0)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': True,
                'execution_time': execution_time,
                'mean_return': float(mean_return),
                'std_return': float(std_return),
                'skewness': float(skewness),
                'kurtosis': float(kurtosis)
            }
            
            print(f"âœ… {test_name} passed - Statistical analysis completed")
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.test_results['test_details'][test_name] = {
                'passed': False,
                'execution_time': execution_time,
                'error': str(e)
            }
            print(f"âŒ {test_name} failed: {e}")
            raise
    
    @classmethod
    def tearDownClass(cls):
        """Limpieza y guardado de resultados"""
        # Calcular estadÃ­sticas finales
        cls.test_results['total_tests'] = len(cls.test_results['test_details'])
        cls.test_results['tests_passed'] = sum(1 for result in cls.test_results['test_details'].values() if result['passed'])
        cls.test_results['tests_failed'] = cls.test_results['total_tests'] - cls.test_results['tests_passed']
        cls.test_results['execution_time'] = sum(result['execution_time'] for result in cls.test_results['test_details'].values())
        
        # Guardar resultados
        results_dir = Path('/workspaces/Sistema-de-datos/Quant/Results Machine Learning/results_labeling')
        results_dir.mkdir(exist_ok=True)
        
        results_file = results_dir / 'test_results.json'
        with open(results_file, 'w') as f:
            json.dump(cls.test_results, f, indent=2)
        
        print(f"\nðŸ“Š Resultados finales guardados en: {results_file}")
        print(f"âœ… Tests pasados: {cls.test_results['tests_passed']}/{cls.test_results['total_tests']}")
        print(f"â±ï¸  Tiempo total: {cls.test_results['execution_time']:.2f}s")

if __name__ == '__main__':
    print("ðŸ§ª Iniciando tests de labeling...")
    unittest.main(verbosity=2)
